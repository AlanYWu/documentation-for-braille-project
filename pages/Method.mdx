Method

# Method

## Long Short-Term Memory (LSTM) Networks
To address the limitations of traditional RNNs (such as gradient vanishing and exploding), we adopt LSTM (Long Short-Term Memory) networks. LSTMs enhance RNNs by introducing three essential components:

Forget Gate:
Prevents gradient explosion and vanishing by selectively retaining relevant historical information.
Allows the model to forget irrelevant past states.

Memory Cell:
Maintains a memory of previous historical information.
Captures long-term dependencies.

Output Gate:
Integrates memory information to produce the final output.
Enables the LSTM to capture long-distance computational relationships.

3. LSTM Network Architecture (Figure 3)
!LSTM Structure Figure 3: Illustration of LSTM Structure

4. Update Equations for Input Data at Time t
$$
Input gate: i_t = \sigma(W_i h_{t-1} + U_i x_t + b_i)
Forget gate: (f_t = \sigma(W_f h_{t-1} + U_f x_t + b_f))


Output gate: (o_t = \sigma(W_o h_{t-1} + U_o x_t + b_o))
Hidden state: (h_t = o_t \odot \tanh(c_t))
Where:
$$

(\sigma) represents the sigmoid function.
(\odot) denotes element-wise multiplication.
(x_t) is the input vector at time (t), which in our case is the word-embedded input vector.
(h_t) represents the hidden state, incorporating all previous states up to time (t).
(U) and (W) are the weights and biases for each layer.





